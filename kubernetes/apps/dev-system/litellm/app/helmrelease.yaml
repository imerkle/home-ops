---
# yaml-language-server: $schema=https://raw.githubusercontent.com/bjw-s-labs/helm-charts/main/charts/other/app-template/schemas/helmrelease-helm-v2.schema.json
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: litellm
spec:
  interval: 1h
  chartRef:
    kind: OCIRepository
    name: app-template
  values:
    controllers:
      litellm:
        annotations:
          reloader.stakater.com/auto: "true"
        containers:
          app:
            image:
              repository: ghcr.io/berriai/litellm
              tag:  v1.81.0-stable.1
            args:
              - --port
              - "4000"
              - --config
              - /app/config.yaml
            env:
              LITELLM_MASTER_KEY: "vault:secret/data/ai#LITELLM_MASTER_KEY"
              OPENAI_API_KEY: "vault:secret/data/ai#OPENAI_API_KEY"
              ANTHROPIC_API_KEY: "vault:secret/data/ai#ANTHROPIC_API_KEY"
              VERTEX_PROJECT: "vault:secret/data/ai#VERTEX_PROJECT"
              VERTEX_LOCATION: "vault:secret/data/ai#VERTEX_LOCATION"
              GOOGLE_APPLICATION_CREDENTIALS: /secrets/google-sa/credentials.json
            probes:
              liveness:
                enabled: true
                custom: true
                spec:
                  httpGet:
                    path: /health/liveliness
                    port: &port 4000
                  initialDelaySeconds: 30
                  periodSeconds: 60
                  timeoutSeconds: 10
                  failureThreshold: 3
              readiness:
                enabled: true
                custom: true
                spec:
                  httpGet:
                    path: /health/readiness
                    port: *port
                  initialDelaySeconds: 5
                  periodSeconds: 5
                  timeoutSeconds: 5
                  failureThreshold: 3
            securityContext:
              allowPrivilegeEscalation: false
              readOnlyRootFilesystem: false
              capabilities: { drop: ["ALL"] }
            resources:
              requests:
                cpu: 10m
              limits:
                memory: 1Gi
    service:
      app:
        controller: litellm
        ports:
          http:
            port: *port
    route:
      app:
        enabled: true
        parentRefs:
          - name: envoy-internal
            namespace: network
        hostnames: ["ai.x3y.space"]
        rules:
          - backendRefs:
              - port: *port
                name: litellm-app
    persistence:
      config:
        type: configMap
        name: litellm-config
        globalMounts:
          - path: /app/config.yaml
            subPath: config.yaml
            readOnly: true
